{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5634a06",
   "metadata": {},
   "source": [
    "在之前的学习中，我们在模型部署上顺风顺水，没有碰到任何问题。这是因为 SRCNN 模型只包含几个简单的算子，而这些卷积、插值算子已经在各个中间表示和推理引擎上得到了完美支持。如果模型的操作稍微复杂一点，我们可能就要为兼容模型而付出大量的功夫了。实际上，模型部署时一般会碰到以下几类困难：\n",
    "\n",
    "模型的动态化。出于性能的考虑，各推理框架都默认模型的输入形状、输出形状、结构是静态的。而为了让模型的泛用性更强，部署时需要在尽可能不影响原有逻辑的前提下，让模型的输入输出或是结构动态化。\n",
    "新算子的实现。深度学习技术日新月异，提出新算子的速度往往快于 ONNX 维护者支持的速度。为了部署最新的模型，部署工程师往往需要自己在 ONNX 和推理引擎中支持新算子。\n",
    "中间表示与推理引擎的兼容问题。由于各推理引擎的实现不同，对 ONNX 难以形成统一的支持。为了确保模型在不同的推理引擎中有同样的运行效果，部署工程师往往得为某个推理引擎定制模型代码，这为模型部署引入了许多工作量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631a8613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.nn.functional import interpolate \n",
    "import torch.onnx \n",
    "import cv2 \n",
    "import numpy as np \n",
    " \n",
    " \n",
    "class SuperResolutionNet(nn.Module): \n",
    " \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    " \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4) \n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0) \n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2) \n",
    " \n",
    "        self.relu = nn.ReLU() \n",
    " \n",
    "    def forward(self, x, upscale_factor): \n",
    "        x = interpolate(x, \n",
    "                        scale_factor=upscale_factor, \n",
    "                        mode='bicubic', \n",
    "                        align_corners=False) \n",
    "        out = self.relu(self.conv1(x)) \n",
    "        out = self.relu(self.conv2(out)) \n",
    "        out = self.conv3(out) \n",
    "        return out \n",
    " \n",
    " \n",
    "def init_torch_model(): \n",
    "    torch_model = SuperResolutionNet() \n",
    " \n",
    "    state_dict = torch.load('srcnn.pth')['state_dict'] \n",
    " \n",
    "    # Adapt the checkpoint \n",
    "    for old_key in list(state_dict.keys()): \n",
    "        new_key = '.'.join(old_key.split('.')[1:]) \n",
    "        state_dict[new_key] = state_dict.pop(old_key) \n",
    " \n",
    "    torch_model.load_state_dict(state_dict) \n",
    "    torch_model.eval() \n",
    "    return torch_model \n",
    " \n",
    " \n",
    "model = init_torch_model() \n",
    " \n",
    "input_img = cv2.imread('face.png').astype(np.float32) \n",
    " \n",
    "# HWC to NCHW \n",
    "input_img = np.transpose(input_img, [2, 0, 1]) \n",
    "input_img = np.expand_dims(input_img, 0) \n",
    " \n",
    "# Inference \n",
    "torch_output = model(torch.from_numpy(input_img), 2).detach().numpy()    #在这里控制传参\n",
    " \n",
    "# NCHW to HWC \n",
    "torch_output = np.squeeze(torch_output, 0) \n",
    "torch_output = np.clip(torch_output, 0, 255) \n",
    "torch_output = np.transpose(torch_output, [1, 2, 0]).astype(np.uint8) \n",
    " \n",
    "# Show image \n",
    "cv2.imwrite(\"face_torch_2.png\", torch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f202e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24624\\453714066.py:4: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(model, (x, 3),\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "upsample_bicubic2d() received an invalid combination of arguments - got (Tensor, NoneType, bool, list), but expected one of:\n * (Tensor input, tuple of ints output_size, bool align_corners, tuple of floats scale_factors)\n      didn't match because some of the arguments have invalid types: (Tensor, !NoneType!, bool, !list of [Tensor, Tensor]!)\n * (Tensor input, tuple of ints output_size, bool align_corners, float scales_h = None, float scales_w = None, *, Tensor out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m x = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m) \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msrcnn2.onnx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                      \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfactor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                      \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\onnx\\__init__.py:424\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes:\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    420\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe exporter only supports dynamic shapes \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthrough parameter dynamic_axes when dynamo=False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\onnx\\utils.py:522\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    520\u001b[39m     args = args + (kwargs,)\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\onnx\\utils.py:1457\u001b[39m, in \u001b[36m_export\u001b[39m\u001b[34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[39m\n\u001b[32m   1454\u001b[39m     dynamic_axes = {}\n\u001b[32m   1455\u001b[39m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[32m-> \u001b[39m\u001b[32m1457\u001b[39m graph, params_dict, torch_out = \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m custom_opsets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1471\u001b[39m     custom_opsets = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\onnx\\utils.py:1080\u001b[39m, in \u001b[36m_model_to_graph\u001b[39m\u001b[34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[39m\n\u001b[32m   1077\u001b[39m     args = (args,)\n\u001b[32m   1079\u001b[39m model = _pre_trace_quant_model(model, args)\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m graph, params, torch_out, module = \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m params_dict = _get_named_param_dict(graph, params)\n\u001b[32m   1083\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\onnx\\utils.py:964\u001b[39m, in \u001b[36m_create_jit_graph\u001b[39m\u001b[34m(model, args)\u001b[39m\n\u001b[32m    959\u001b[39m     graph = _C._propagate_and_assign_input_shapes(\n\u001b[32m    960\u001b[39m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    961\u001b[39m     )\n\u001b[32m    962\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m graph, torch_out = \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m _C._jit_pass_onnx_lint(graph)\n\u001b[32m    966\u001b[39m state_dict = torch.jit._unique_state_dict(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\onnx\\utils.py:871\u001b[39m, in \u001b[36m_trace_and_get_graph_from_model\u001b[39m\u001b[34m(model, args)\u001b[39m\n\u001b[32m    869\u001b[39m prev_autocast_cache_enabled = torch.is_autocast_cache_enabled()\n\u001b[32m    870\u001b[39m torch.set_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m trace_graph, torch_out, inputs_states = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    878\u001b[39m torch.set_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[32m    880\u001b[39m warn_on_static_input_change(inputs_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\jit\\_trace.py:1504\u001b[39m, in \u001b[36m_get_trace_graph\u001b[39m\u001b[34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[39m\n\u001b[32m   1502\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1503\u001b[39m     args = (args,)\n\u001b[32m-> \u001b[39m\u001b[32m1504\u001b[39m outs = \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\jit\\_trace.py:138\u001b[39m, in \u001b[36mONNXTracedModule.forward\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m graph, _out = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return_inputs:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[32m0\u001b[39m], ret_inputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\jit\\_trace.py:129\u001b[39m, in \u001b[36mONNXTracedModule.forward.<locals>.wrapper\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return_inputs_states:\n\u001b[32m    128\u001b[39m     inputs_states.append(_unflatten(in_args, in_desc))\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m outs.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return_inputs_states:\n\u001b[32m    131\u001b[39m     inputs_states[\u001b[32m0\u001b[39m] = (inputs_states[\u001b[32m0\u001b[39m], trace_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1763\u001b[39m, in \u001b[36mModule._slow_forward\u001b[39m\u001b[34m(self, *input, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m         recording_scopes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1763\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mSuperResolutionNet.forward\u001b[39m\u001b[34m(self, x, upscale_factor)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, upscale_factor): \n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     x = \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbicubic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                    \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[32m     25\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.conv1(x)) \n\u001b[32m     26\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.conv2(out)) \n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\deep learing\\ANACONDA\\envs\\onnx\\Lib\\site-packages\\torch\\nn\\functional.py:4788\u001b[39m, in \u001b[36minterpolate\u001b[39m\u001b[34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[39m\n\u001b[32m   4784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[32m   4785\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn._upsample_bicubic2d_aa(\n\u001b[32m   4786\u001b[39m             \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors\n\u001b[32m   4787\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsample_bicubic2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4789\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[32m   4790\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mbilinear\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   4793\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mGot 3D input, but bilinear mode needs 4D input\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: upsample_bicubic2d() received an invalid combination of arguments - got (Tensor, NoneType, bool, list), but expected one of:\n * (Tensor input, tuple of ints output_size, bool align_corners, tuple of floats scale_factors)\n      didn't match because some of the arguments have invalid types: (Tensor, !NoneType!, bool, !list of [Tensor, Tensor]!)\n * (Tensor input, tuple of ints output_size, bool align_corners, float scales_h = None, float scales_w = None, *, Tensor out = None)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 256, 256) \n",
    " \n",
    "with torch.no_grad(): \n",
    "    torch.onnx.export(model, (x, 3), \n",
    "                      \"srcnn2.onnx\", \n",
    "                      opset_version=11, \n",
    "                      input_names=['input', 'factor'], \n",
    "                      output_names=['output']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8cfab",
   "metadata": {},
   "source": [
    "直接使用 PyTorch 模型的话，我们修改几行代码就能实现模型输入的动态化。但在模型部署中，我们要花数倍的时间来设法解决这一问题。现在，让我们顺着解决问题的思路，体验一下模型部署的困难，并学习使用自定义算子的方式，解决超分辨率模型的动态化问题。\n",
    "\n",
    "刚刚的报错是因为 PyTorch 模型在导出到 ONNX 模型时，模型的输入参数的类型必须全部是 torch.Tensor。而实际上我们传入的第二个参数\" 3 \"是一个整形变量。这不符合 PyTorch 转 ONNX 的规定。我们必须要修改一下原来的模型的输入。为了保证输入的所有参数都是 torch.Tensor 类型的，我们做如下修改："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb13d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24624\\1235139919.py:61: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(model, (x, torch.tensor(3)),\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24624\\1235139919.py:14: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  scale_factor=upscale_factor.item(),\n"
     ]
    }
   ],
   "source": [
    "class SuperResolutionNet(nn.Module): \n",
    " \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    " \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4) \n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0) \n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2) \n",
    " \n",
    "        self.relu = nn.ReLU() \n",
    " \n",
    "    def forward(self, x, upscale_factor): \n",
    "        x = interpolate(x, \n",
    "                        scale_factor=upscale_factor.item(), \n",
    "                        mode='bicubic', \n",
    "                        align_corners=False) \n",
    "        out = self.relu(self.conv1(x)) \n",
    "        out = self.relu(self.conv2(out)) \n",
    "        out = self.conv3(out) \n",
    "        return out \n",
    " \n",
    " \n",
    "def init_torch_model(): \n",
    "    torch_model = SuperResolutionNet() \n",
    " \n",
    "    state_dict = torch.load('srcnn.pth')['state_dict'] \n",
    " \n",
    "    # Adapt the checkpoint \n",
    "    for old_key in list(state_dict.keys()): \n",
    "        new_key = '.'.join(old_key.split('.')[1:]) \n",
    "        state_dict[new_key] = state_dict.pop(old_key) \n",
    " \n",
    "    torch_model.load_state_dict(state_dict) \n",
    "    torch_model.eval() \n",
    "    return torch_model \n",
    " \n",
    " \n",
    "model = init_torch_model() \n",
    " \n",
    "input_img = cv2.imread('face.png').astype(np.float32) \n",
    " \n",
    "# HWC to NCHW \n",
    "input_img = np.transpose(input_img, [2, 0, 1]) \n",
    "input_img = np.expand_dims(input_img, 0) \n",
    " \n",
    "# Inference \n",
    "torch_output = model(torch.from_numpy(input_img), torch.tensor(3)).detach().numpy()    #在这里控制传参\n",
    " \n",
    "# NCHW to HWC \n",
    "torch_output = np.squeeze(torch_output, 0) \n",
    "torch_output = np.clip(torch_output, 0, 255) \n",
    "torch_output = np.transpose(torch_output, [1, 2, 0]).astype(np.uint8) \n",
    " \n",
    "# Show image \n",
    "cv2.imwrite(\"face_torch_2.png\", torch_output)\n",
    "\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256) \n",
    " \n",
    "with torch.no_grad(): \n",
    "    torch.onnx.export(model, (x, torch.tensor(3)), \n",
    "                      \"srcnn2.onnx\", \n",
    "                      opset_version=11, \n",
    "                      input_names=['input', 'factor'], \n",
    "                      output_names=['output']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690e6fb",
   "metadata": {},
   "source": [
    "可以发现，虽然我们把模型推理的输入设置为了两个，但 ONNX 模型还是长得和原来一模一样，只有一个叫 \" input \" 的输入。这是由于我们使用了 torch.Tensor.item() 把数据从 Tensor 里取出来，而导出 ONNX 模型时这个操作是无法被记录的，只好报了一条 TraceWarning。这导致 interpolate 插值函数的放大倍数还是被设置成了\" 3 \"这个固定值，我们导出的\" srcnn2.onnx \"和最开始的\" srcnn.onnx \"完全相同。\n",
    "\n",
    "直接修改原来的模型似乎行不通，我们得从 PyTorch 转 ONNX 的原理入手，强行令 ONNX 模型明白我们的想法了。\n",
    "\n",
    "仔细观察 Netron 上可视化出的 ONNX 模型，可以发现在 PyTorch 中无论是使用最早的 nn.Upsample，还是后来的 interpolate，PyTorch 里的插值操作最后都会转换成 ONNX 定义的 Resize 操作。也就是说，所谓 PyTorch 转 ONNX，实际上就是把每个 PyTorch 的操作映射成了 ONNX 定义的算子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760119cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewInterpolate(torch.autograd.Function):    #继承这个类，用来重写前向实现及导出到 ONNX 时的符号化逻辑\n",
    " \n",
    "    @staticmethod \n",
    "    def symbolic(g, input, scales):            #在导出 ONNX 时调用\n",
    "        return g.op(\"Resize\",                  #使用ONNX Resize算子替换interpolate函数 \n",
    "                    input, \n",
    "                    g.op(\"Constant\", \n",
    "                         value_t=torch.tensor([], dtype=torch.float32)), \n",
    "                    scales, \n",
    "                    coordinate_transformation_mode_s=\"pytorch_half_pixel\", \n",
    "                    cubic_coeff_a_f=-0.75, \n",
    "                    mode_s='cubic', \n",
    "                    nearest_mode_s=\"floor\") \n",
    " \n",
    "    @staticmethod \n",
    "    def forward(ctx, input, scales): \n",
    "        scales = scales.tolist()[-2:] \n",
    "        return interpolate(input, \n",
    "                           scale_factor=scales, \n",
    "                           mode='bicubic', \n",
    "                           align_corners=False) \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1908cdc",
   "metadata": {},
   "source": [
    "我们希望新的插值算子有两个输入，一个是被用于操作的图像，一个是图像的放缩比例。前面讲到，为了对接 ONNX 中 Resize 算子的 scales 参数，这个放缩比例是一个 [1, 1, x, x] 的张量，其中 x 为放大倍数。在之前放大3倍的模型中，这个参数被固定成了[1, 1, 3, 3]。因此，在插值算子中，我们希望模型的第二个输入是一个 [1, 1, w, h] 的张量，其中 w 和 h 分别是图片宽和高的放大倍数。\n",
    "\n",
    "搞清楚了插值算子的输入，再看一看算子的具体实现。算子的推理行为由算子的 foward 方法决定。该方法的第一个参数必须为 ctx，后面的参数为算子的自定义输入，我们设置两个输入，分别为被操作的图像和放缩比例。为保证推理正确，需要把 [1, 1, w, h] 格式的输入对接到原来的 interpolate 函数上。我们的做法是截取输入张量的后两个元素，把这两个元素以 list 的格式传入 interpolate 的 scale_factor 参数。\n",
    "\n",
    "接下来，我们要决定新算子映射到 ONNX 算子的方法。映射到 ONNX 的方法由一个算子的 symbolic 方法决定。symbolic 方法第一个参数必须是g，之后的参数是算子的自定义输入，和 forward 函数一样。ONNX 算子的具体定义由 g.op 实现。g.op 的每个参数都可以映射到 ONNX 中的算子属性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69660883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StrangeSuperResolutionNet(nn.Module): \n",
    " \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    " \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4) \n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0) \n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2) \n",
    " \n",
    "        self.relu = nn.ReLU() \n",
    " \n",
    "    def forward(self, x, upscale_factor): \n",
    "        x = NewInterpolate.apply(x, upscale_factor) \n",
    "        out = self.relu(self.conv1(x)) \n",
    "        out = self.relu(self.conv2(out)) \n",
    "        out = self.conv3(out) \n",
    "        return out \n",
    " \n",
    " \n",
    "def init_torch_model(): \n",
    "    torch_model = StrangeSuperResolutionNet() \n",
    " \n",
    "    state_dict = torch.load('srcnn.pth')['state_dict'] \n",
    " \n",
    "    # Adapt the checkpoint \n",
    "    for old_key in list(state_dict.keys()): \n",
    "        new_key = '.'.join(old_key.split('.')[1:]) \n",
    "        state_dict[new_key] = state_dict.pop(old_key) \n",
    " \n",
    "    torch_model.load_state_dict(state_dict) \n",
    "    torch_model.eval() \n",
    "    return torch_model \n",
    " \n",
    " \n",
    "model = init_torch_model() \n",
    "factor = torch.tensor([1, 1, 3, 3], dtype=torch.float) \n",
    " \n",
    "input_img = cv2.imread('face.png').astype(np.float32)     #这里读到的是 256x256x3 的图像     \n",
    " \n",
    "# HWC to NCHW \n",
    "input_img = np.transpose(input_img, [2, 0, 1])            #转换为 3x256x256\n",
    "input_img = np.expand_dims(input_img, 0)                  #增加一个维度，变为 1x3x256x256\n",
    " \n",
    "# Inference \n",
    "torch_output = model(torch.from_numpy(input_img), factor).detach().numpy() \n",
    " \n",
    "# NCHW to HWC \n",
    "torch_output = np.squeeze(torch_output, 0) \n",
    "torch_output = np.clip(torch_output, 0, 255) \n",
    "torch_output = np.transpose(torch_output, [1, 2, 0]).astype(np.uint8) \n",
    " \n",
    "# Show image \n",
    "cv2.imwrite(\"face_torch_3.png\", torch_output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6fee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24624\\2924835335.py:4: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(model, (x, factor),\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24624\\1601210842.py:17: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  scales = scales.tolist()[-2:]\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 256, 256) \n",
    " \n",
    "with torch.no_grad(): \n",
    "    torch.onnx.export(model, (x, factor), \n",
    "                      \"srcnn3.onnx\", \n",
    "                      opset_version=11, \n",
    "                      input_names=['input', 'factor'], \n",
    "                      output_names=['output']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5210b712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime \n",
    "\n",
    "input_factor = np.array([1, 1, 5, 5], dtype=np.float32) \n",
    "ort_session = onnxruntime.InferenceSession(\"srcnn3.onnx\") \n",
    "ort_inputs = {'input': input_img, 'factor': input_factor} \n",
    "ort_output = ort_session.run(None, ort_inputs)[0]\n",
    "\n",
    "ort_output = np.squeeze(ort_output, 0) \n",
    "ort_output = np.clip(ort_output, 0, 255) \n",
    "ort_output = np.transpose(ort_output, [1, 2, 0]).astype(np.uint8) \n",
    "cv2.imwrite(\"face_ort_3.png\", ort_output) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
